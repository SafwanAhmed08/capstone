{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in /Users/safwanahmed/Desktop/capstone/.venv/lib/python3.9/site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in /Users/safwanahmed/Desktop/capstone/.venv/lib/python3.9/site-packages (from imblearn) (0.12.4)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/safwanahmed/Desktop/capstone/.venv/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/safwanahmed/Desktop/capstone/.venv/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /Users/safwanahmed/Desktop/capstone/.venv/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (1.1.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/safwanahmed/Desktop/capstone/.venv/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/safwanahmed/Desktop/capstone/.venv/lib/python3.9/site-packages (from imbalanced-learn->imblearn) (3.5.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'arp.opcode', 'arp.hw.size', 'icmp.checksum',\n",
       "       'icmp.seq_le', 'http.content_length', 'http.response', 'tcp.ack',\n",
       "       'tcp.ack_raw', 'tcp.checksum', 'tcp.connection.fin',\n",
       "       'tcp.connection.rst', 'tcp.connection.syn', 'tcp.connection.synack',\n",
       "       'tcp.dstport', 'tcp.flags', 'tcp.flags.ack', 'tcp.len', 'tcp.seq',\n",
       "       'tcp.srcport', 'udp.stream', 'udp.time_delta', 'dns.qry.name',\n",
       "       'dns.qry.qu', 'dns.retransmission', 'dns.retransmit_request',\n",
       "       'mqtt.conflag.cleansess', 'mqtt.conflags', 'mqtt.hdrflags', 'mqtt.len',\n",
       "       'mqtt.msgtype', 'mqtt.proto_len', 'mqtt.topic_len', 'mqtt.ver',\n",
       "       'Attack_label', 'Attack_type', 'http.request.method_0',\n",
       "       'http.request.method_0.0', 'http.request.method_GET',\n",
       "       'http.request.method_OPTIONS', 'http.request.method_POST',\n",
       "       'http.request.method_PROPFIND', 'http.request.method_PUT',\n",
       "       'http.request.method_SEARCH', 'http.request.method_TRACE',\n",
       "       'http.referer_() { _; } >_[$($())] { echo 93e4r0-CVE-2014-6278: true; echo;echo; }',\n",
       "       'http.referer_0', 'http.referer_0.0', 'http.referer_127.0.0.1',\n",
       "       'http.referer_TESTING_PURPOSES_ONLY',\n",
       "       'http.request.version_-a HTTP/1.1',\n",
       "       'http.request.version_-al&ABSOLUTE_PATH_STUDIP=http://cirt.net/rfiinc.txt?? HTTP/1.1',\n",
       "       'http.request.version_-al&_PHPLIB[libdir]=http://cirt.net/rfiinc.txt?? HTTP/1.1',\n",
       "       'http.request.version_/etc/passwd|?data=Download HTTP/1.1',\n",
       "       'http.request.version_0', 'http.request.version_0.0',\n",
       "       'http.request.version_> HTTP/1.1',\n",
       "       'http.request.version_By Dr HTTP/1.1', 'http.request.version_HTTP/1.0',\n",
       "       'http.request.version_HTTP/1.1',\n",
       "       'http.request.version_Src=javascript:alert('Vulnerable')><Img Src=\\\" HTTP/1.1',\n",
       "       'http.request.version_name=a><input name=i value=XSS>&lt;script>alert('Vulnerable')</script> HTTP/1.1',\n",
       "       'http.request.version_script>alert(1)/script><\\\" HTTP/1.1',\n",
       "       'dns.qry.name.len_0', 'dns.qry.name.len_0.0',\n",
       "       'dns.qry.name.len_0.debian.pool.ntp.org', 'dns.qry.name.len_1.0',\n",
       "       'dns.qry.name.len_1.debian.pool.ntp.org',\n",
       "       'dns.qry.name.len_2.debian.pool.ntp.org',\n",
       "       'dns.qry.name.len_3.debian.pool.ntp.org',\n",
       "       'dns.qry.name.len__googlecast._tcp.local',\n",
       "       'dns.qry.name.len_null-null.local',\n",
       "       'dns.qry.name.len_raspberrypi.local', 'mqtt.conack.flags_0',\n",
       "       'mqtt.conack.flags_0.0', 'mqtt.conack.flags_0x00000000',\n",
       "       'mqtt.conack.flags_1461073', 'mqtt.conack.flags_1461074',\n",
       "       'mqtt.conack.flags_1461383', 'mqtt.conack.flags_1461384',\n",
       "       'mqtt.conack.flags_1461589', 'mqtt.conack.flags_1461591',\n",
       "       'mqtt.conack.flags_1471198', 'mqtt.conack.flags_1471199',\n",
       "       'mqtt.conack.flags_1574358', 'mqtt.conack.flags_1574359',\n",
       "       'mqtt.protoname_0', 'mqtt.protoname_0.0', 'mqtt.protoname_MQTT',\n",
       "       'mqtt.topic_0', 'mqtt.topic_0.0', 'mqtt.topic_Temperature_and_Humidity',\n",
       "       'Attack_class', 'uri_query_len', 'uri_query_entropy',\n",
       "       'uri_special_char_count', 'file_data_len', 'file_data_entropy',\n",
       "       'file_data_special_chars', 'tcp_option_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../dataset/processed_dataset.csv\", low_memory=False)\n",
    "df = df.dropna()\n",
    "\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature scaling complete!\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "df = df.drop(columns=[\"Attack_label\", \"Attack_class\"])\n",
    "X_scaled = scaler.fit_transform(df.drop(columns=[\"Attack_type\"]))\n",
    "y = df[\"Attack_type\"]\n",
    "print(\"Feature scaling complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features: 97, Reduced features: 64\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=0.99)  # Increased from 0.95 to 0.99\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "joblib.dump(pca,'pca.pkl')\n",
    "print(f\"Original features: {X_scaled.shape[1]}, Reduced features: {X_pca.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (1299563, 64), Validation set: (324891, 64), Testing set: (406114, 64)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "print(f\"Training set: {X_train.shape}, Validation set: {X_val.shape}, Testing set: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled training set: (12862388, 64)\n"
     ]
    }
   ],
   "source": [
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "print(f\"Resampled training set: {X_train_resampled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New class distribution after targeted SMOTE:\n",
      "{'Normal': 918742, 'DDoS_ICMP': 74516, 'DDoS_HTTP': 31895, 'Uploading': 23832, 'Password': 50000, 'SQL_injection': 32623, 'DDoS_UDP': 77803, 'Backdoor': 15382, 'DDoS_TCP': 32040, 'Port_Scanning': 50000, 'Vulnerability_scanner': 32018, 'Ransomware': 6203, 'XSS': 50000, 'MITM': 50000}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['label_encoder.pkl']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert labels to string for encoding\n",
    "y_train_str = y_train.astype(str)\n",
    "y_val_str = y_val.astype(str)\n",
    "y_test_str = y_test.astype(str)\n",
    "\n",
    "# Combine all label sets to preserve unseen classes for encoding\n",
    "combined_labels = np.concatenate([y_train_str, y_val_str, y_test_str])\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(combined_labels)\n",
    "\n",
    "# Transform labels to integers\n",
    "y_train_encoded = label_encoder.transform(y_train_str)\n",
    "y_val_encoded = label_encoder.transform(y_val_str)\n",
    "y_test_encoded = label_encoder.transform(y_test_str)\n",
    "\n",
    "# Create a mapping of label name to encoded value\n",
    "label_to_id = {name: idx for idx, name in enumerate(label_encoder.classes_)}\n",
    "id_to_label = {idx: name for name, idx in label_to_id.items()}\n",
    "\n",
    "# Select underperforming classes to boost\n",
    "underperforming = ['XSS', 'MITM', 'Password', 'Port_Scanning']\n",
    "\n",
    "# Target number of samples for each\n",
    "smote_strategy = {\n",
    "    label_to_id[label]: 50000 for label in underperforming\n",
    "}\n",
    "\n",
    "# Apply SMOTE only to selected classes\n",
    "smote = SMOTE(sampling_strategy=smote_strategy, random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train_encoded)\n",
    "\n",
    "# Check counts\n",
    "print(\"New class distribution after targeted SMOTE:\")\n",
    "print({id_to_label[k]: v for k, v in Counter(y_train_resampled).items()})\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(df['Attack_type'])\n",
    "\n",
    "# Save updated label encoder\n",
    "import joblib\n",
    "joblib.dump(label_encoder, 'label_encoder.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     label_encoder = joblib.load(\"label_encoder.pkl\")\n",
    "#     y_train_tensor = torch.load(\"y_train_tensor.pt\")\n",
    "#     y_val_tensor = torch.load(\"y_val_tensor.pt\")\n",
    "#     y_test_tensor = torch.load(\"y_test_tensor.pt\")\n",
    "# except:\n",
    "#     # Create and fit label encoder\n",
    "#     label_encoder = LabelEncoder()\n",
    "#     y_train_resampled_str = y_train_resampled.astype(str)\n",
    "#     y_val_str = y_val.astype(str)\n",
    "#     y_test_str = y_test.astype(str)\n",
    "    \n",
    "#     y_train_encoded = label_encoder.fit_transform(y_train_resampled_str)\n",
    "#     y_val_encoded = label_encoder.transform(y_val_str)\n",
    "#     y_test_encoded = label_encoder.transform(y_test_str)\n",
    "    \n",
    "#     # Convert to tensors\n",
    "#     y_train_tensor = torch.tensor(y_train_encoded, dtype=torch.long)\n",
    "#     y_val_tensor = torch.tensor(y_val_encoded, dtype=torch.long)\n",
    "#     y_test_tensor = torch.tensor(y_test_encoded, dtype=torch.long)\n",
    "    \n",
    "#     # Save for future use\n",
    "#     joblib.dump(label_encoder, \"label_encoder.pkl\")\n",
    "#     torch.save(y_train_tensor, \"y_train_tensor.pt\")\n",
    "#     torch.save(y_val_tensor, \"y_val_tensor.pt\")\n",
    "#     torch.save(y_test_tensor, \"y_test_tensor.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_resampled shape: (1445054, 64)\n",
      "y_train_resampled shape: (1445054,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train_resampled shape:\", X_train_resampled.shape)\n",
    "print(\"y_train_resampled shape:\", y_train_resampled.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tensor = torch.tensor(y_train_resampled, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val_encoded, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_encoded, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(y_train_tensor, \"y_train_tensor.pt\")\n",
    "torch.save(y_val_tensor, \"y_val_tensor.pt\")\n",
    "torch.save(y_test_tensor, \"y_test_tensor.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train_resampled, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(X_train_tensor, \"X_train_tensor.pt\")\n",
    "torch.save(X_val_tensor, \"X_val_tensor.pt\")\n",
    "torch.save(X_test_tensor, \"X_test_tensor.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_resampled shape: torch.Size([1445054, 64])\n",
      "y_train_resampled shape: torch.Size([1445054])\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train_resampled shape:\", X_train_tensor.shape)\n",
    "print(\"y_train_resampled shape:\", y_train_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_classes = np.unique(y_train_resampled)\n",
    "class_weights = compute_class_weight('balanced', classes=unique_classes, y=y_train_resampled)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512  # Increased from 128 to 512\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulation_steps = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedDNN(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, dropout_rate=0.3):\n",
    "        super(ImprovedDNN, self).__init__()\n",
    "        \n",
    "        # Wider architecture with batch normalization\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.bn4 = nn.BatchNorm1d(64)\n",
    "        self.fc5 = nn.Linear(64, num_classes)\n",
    "\n",
    "        # Activation & regularization\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.act = nn.LeakyReLU(0.1)  # LeakyReLU instead of ReLU\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.act(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.act(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.act(self.bn4(self.fc4(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 14\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_train_resampled.shape[1]\n",
    "num_classes = len(set(y_train_resampled))\n",
    "print(input_dim,num_classes)\n",
    "\n",
    "model = ImprovedDNN(input_dim, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 2823/2823 [00:18<00:00, 156.24batch/s, accuracy=0.991, loss=0.0494]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1] - Loss: 0.0661 - Train Accuracy: 0.9915\n",
      "Validation Loss: 0.0496 - Validation Accuracy: 0.9944\n",
      "Full model saved with validation accuracy: 0.9944\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "best_val_acc = 0\n",
    "patience = 1\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    optimizer.zero_grad()  # Reset gradients at the beginning of epoch\n",
    "    \n",
    "    with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\") as tepoch:\n",
    "        for i, (X_batch, y_batch) in enumerate(tepoch):\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            \n",
    "            # Normalize loss to account for accumulation\n",
    "            loss = loss / accumulation_steps\n",
    "            loss.backward()\n",
    "            \n",
    "            # Only step and zero_grad after accumulation_steps\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                # Add gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            running_loss += loss.item() * accumulation_steps\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "            \n",
    "            tepoch.set_postfix(loss=loss.item() * accumulation_steps, accuracy=correct/total)\n",
    "    \n",
    "    # Step optimizer for remaining gradients\n",
    "    if len(train_loader) % accumulation_steps != 0:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    train_acc = correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {running_loss/len(train_loader):.4f} - Train Accuracy: {train_acc:.4f}\")\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    val_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in val_loader:\n",
    "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "            outputs = model(X_val)\n",
    "            loss = criterion(outputs, y_val)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += y_val.size(0)\n",
    "            val_correct += (predicted == y_val).sum().item()\n",
    "    \n",
    "    val_acc = val_correct / val_total\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    print(f\"Validation Loss: {val_loss:.4f} - Validation Accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model, 'best_model.pth')\n",
    "        print(f\"Full model saved with validation accuracy: {best_val_acc:.4f}\")\n",
    "        patience_counter = 0\n",
    "\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "# Load the best model for evaluation\n",
    "model = torch.load('best_model.pth',weights_only = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImprovedDNN(\n",
       "  (fc1): Linear(in_features=64, out_features=512, bias=True)\n",
       "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc4): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (bn4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc5): Linear(in_features=64, out_features=14, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (act): LeakyReLU(negative_slope=0.1)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, device, label_encoder):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_test, y_test in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "            outputs = model(X_test)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(y_test.cpu().numpy())\n",
    "\n",
    "    class_names = label_encoder.classes_\n",
    "\n",
    "    # Safely map predictions to class names\n",
    "    pred_classes = [class_names[int(i)] for i in all_preds]\n",
    "    target_classes = [class_names[int(i)] for i in all_targets]\n",
    "\n",
    "    print(\"\\n📊 Classification Report:\")\n",
    "    print(classification_report(target_classes, pred_classes))\n",
    "\n",
    "    cm = confusion_matrix(target_classes, pred_classes)\n",
    "    return cm, classification_report(target_classes, pred_classes, output_dict=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 794/794 [00:01<00:00, 456.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Classification Report:\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "             Backdoor       1.00      0.98      0.99      4807\n",
      "            DDoS_HTTP       0.96      0.89      0.93      9967\n",
      "            DDoS_ICMP       1.00      1.00      1.00     23286\n",
      "             DDoS_TCP       0.98      0.92      0.95     10012\n",
      "             DDoS_UDP       1.00      1.00      1.00     24313\n",
      "                 MITM       1.00      0.86      0.92         7\n",
      "               Normal       1.00      1.00      1.00    287107\n",
      "             Password       1.00      1.00      1.00     10016\n",
      "        Port_Scanning       0.82      1.00      0.90      3996\n",
      "           Ransomware       1.00      0.97      0.98      1939\n",
      "        SQL_injection       1.00      1.00      1.00     10195\n",
      "            Uploading       1.00      1.00      1.00      7447\n",
      "Vulnerability_scanner       1.00      1.00      1.00     10006\n",
      "                  XSS       0.72      0.88      0.79      3016\n",
      "\n",
      "             accuracy                           0.99    406114\n",
      "            macro avg       0.96      0.96      0.96    406114\n",
      "         weighted avg       0.99      0.99      0.99    406114\n",
      "\n",
      "Test evaluation complete!\n",
      "Model and evaluation results saved to disk.\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating model on test set...\")\n",
    "cm, report = evaluate(model, test_loader, device, label_encoder)\n",
    "print(\"Test evaluation complete!\")\n",
    "\n",
    "# Save the model and results\n",
    "torch.save(model.state_dict(), 'final_model.pt')\n",
    "joblib.dump(report, 'classification_report.pkl')\n",
    "print(\"Model and evaluation results saved to disk.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
