{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset/processed_dataset.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature scaling complete!\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "df = df.drop(columns=[\"Attack_label\", \"Attack_class\"])\n",
    "X_scaled = scaler.fit_transform(df.drop(columns=[\"Attack_type\"]))\n",
    "y = df[\"Attack_type\"]\n",
    "print(\"Feature scaling complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features: 84, Reduced features: 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/safwanahmed/Desktop/capstone/DNN/.venv/lib/python3.9/site-packages/sklearn/decomposition/_pca.py:606: RuntimeWarning: divide by zero encountered in matmul\n",
      "  C = X.T @ X\n",
      "/Users/safwanahmed/Desktop/capstone/DNN/.venv/lib/python3.9/site-packages/sklearn/decomposition/_pca.py:606: RuntimeWarning: overflow encountered in matmul\n",
      "  C = X.T @ X\n",
      "/Users/safwanahmed/Desktop/capstone/DNN/.venv/lib/python3.9/site-packages/sklearn/decomposition/_pca.py:606: RuntimeWarning: invalid value encountered in matmul\n",
      "  C = X.T @ X\n",
      "/Users/safwanahmed/Desktop/capstone/DNN/.venv/lib/python3.9/site-packages/sklearn/decomposition/_base.py:148: RuntimeWarning: divide by zero encountered in matmul\n",
      "  X_transformed = X @ self.components_.T\n",
      "/Users/safwanahmed/Desktop/capstone/DNN/.venv/lib/python3.9/site-packages/sklearn/decomposition/_base.py:148: RuntimeWarning: overflow encountered in matmul\n",
      "  X_transformed = X @ self.components_.T\n",
      "/Users/safwanahmed/Desktop/capstone/DNN/.venv/lib/python3.9/site-packages/sklearn/decomposition/_base.py:148: RuntimeWarning: invalid value encountered in matmul\n",
      "  X_transformed = X @ self.components_.T\n",
      "/Users/safwanahmed/Desktop/capstone/DNN/.venv/lib/python3.9/site-packages/sklearn/decomposition/_base.py:155: RuntimeWarning: divide by zero encountered in matmul\n",
      "  X_transformed -= xp.reshape(self.mean_, (1, -1)) @ self.components_.T\n",
      "/Users/safwanahmed/Desktop/capstone/DNN/.venv/lib/python3.9/site-packages/sklearn/decomposition/_base.py:155: RuntimeWarning: overflow encountered in matmul\n",
      "  X_transformed -= xp.reshape(self.mean_, (1, -1)) @ self.components_.T\n",
      "/Users/safwanahmed/Desktop/capstone/DNN/.venv/lib/python3.9/site-packages/sklearn/decomposition/_base.py:155: RuntimeWarning: invalid value encountered in matmul\n",
      "  X_transformed -= xp.reshape(self.mean_, (1, -1)) @ self.components_.T\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=0.99)  # Increased from 0.95 to 0.99\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "print(f\"Original features: {X_scaled.shape[1]}, Reduced features: {X_pca.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (1253340, 57), Validation set: (313336, 57), Testing set: (391670, 57)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "print(f\"Training set: {X_train.shape}, Validation set: {X_val.shape}, Testing set: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/safwanahmed/Desktop/capstone/DNN/.venv/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled training set: (13094400, 57)\n"
     ]
    }
   ],
   "source": [
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "print(f\"Resampled training set: {X_train_resampled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    label_encoder = joblib.load(\"label_encoder.pkl\")\n",
    "    y_train_tensor = torch.load(\"y_train_tensor.pt\")\n",
    "    y_val_tensor = torch.load(\"y_val_tensor.pt\")\n",
    "    y_test_tensor = torch.load(\"y_test_tensor.pt\")\n",
    "except:\n",
    "    # Create and fit label encoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train_resampled_str = y_train_resampled.astype(str)\n",
    "    y_val_str = y_val.astype(str)\n",
    "    y_test_str = y_test.astype(str)\n",
    "    \n",
    "    y_train_encoded = label_encoder.fit_transform(y_train_resampled_str)\n",
    "    y_val_encoded = label_encoder.transform(y_val_str)\n",
    "    y_test_encoded = label_encoder.transform(y_test_str)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    y_train_tensor = torch.tensor(y_train_encoded, dtype=torch.long)\n",
    "    y_val_tensor = torch.tensor(y_val_encoded, dtype=torch.long)\n",
    "    y_test_tensor = torch.tensor(y_test_encoded, dtype=torch.long)\n",
    "    \n",
    "    # Save for future use\n",
    "    joblib.dump(label_encoder, \"label_encoder.pkl\")\n",
    "    torch.save(y_train_tensor, \"y_train_tensor.pt\")\n",
    "    torch.save(y_val_tensor, \"y_val_tensor.pt\")\n",
    "    torch.save(y_test_tensor, \"y_test_tensor.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train_resampled, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_classes = np.unique(y_train_resampled)\n",
    "class_weights = compute_class_weight('balanced', classes=unique_classes, y=y_train_resampled)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512  # Increased from 128 to 512\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulation_steps = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedDNN(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, dropout_rate=0.3):\n",
    "        super(ImprovedDNN, self).__init__()\n",
    "        \n",
    "        # Wider architecture with batch normalization\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.bn4 = nn.BatchNorm1d(64)\n",
    "        self.fc5 = nn.Linear(64, num_classes)\n",
    "\n",
    "        # Activation & regularization\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.act = nn.LeakyReLU(0.1)  # LeakyReLU instead of ReLU\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.act(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.act(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.act(self.bn4(self.fc4(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train_resampled.shape[1]\n",
    "num_classes = len(set(y_train_resampled))\n",
    "\n",
    "model = ImprovedDNN(input_dim, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 25575/25575 [04:45<00:00, 89.54batch/s, accuracy=0.783, loss=0.39]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] - Loss: 0.4509 - Train Accuracy: 0.7830\n",
      "Validation Loss: 0.1123 - Validation Accuracy: 0.9383\n",
      "Model saved with validation accuracy: 0.9383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 25575/25575 [04:39<00:00, 91.53batch/s, accuracy=0.807, loss=0.422] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20] - Loss: 0.3921 - Train Accuracy: 0.8071\n",
      "Validation Loss: 0.1766 - Validation Accuracy: 0.9363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 25575/25575 [04:33<00:00, 93.50batch/s, accuracy=0.808, loss=0.394] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20] - Loss: 0.3882 - Train Accuracy: 0.8084\n",
      "Validation Loss: 0.1066 - Validation Accuracy: 0.9406\n",
      "Model saved with validation accuracy: 0.9406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 25575/25575 [09:31<00:00, 44.73batch/s, accuracy=0.809, loss=0.406]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20] - Loss: 0.3863 - Train Accuracy: 0.8089\n",
      "Validation Loss: 0.1077 - Validation Accuracy: 0.9397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 25575/25575 [04:34<00:00, 93.20batch/s, accuracy=0.809, loss=0.436] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20] - Loss: 0.3853 - Train Accuracy: 0.8091\n",
      "Validation Loss: 0.1478 - Validation Accuracy: 0.9337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 25575/25575 [04:21<00:00, 97.83batch/s, accuracy=0.809, loss=0.421] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20] - Loss: 0.3848 - Train Accuracy: 0.8094\n",
      "Validation Loss: 0.1082 - Validation Accuracy: 0.9395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 25575/25575 [04:35<00:00, 92.80batch/s, accuracy=0.809, loss=0.398]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20] - Loss: 0.3840 - Train Accuracy: 0.8095\n",
      "Validation Loss: 0.1076 - Validation Accuracy: 0.9399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 25575/25575 [04:39<00:00, 91.63batch/s, accuracy=0.811, loss=0.389]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20] - Loss: 0.3798 - Train Accuracy: 0.8109\n",
      "Validation Loss: 0.1090 - Validation Accuracy: 0.9393\n",
      "Early stopping triggered!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "best_val_acc = 0\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    optimizer.zero_grad()  # Reset gradients at the beginning of epoch\n",
    "    \n",
    "    with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\") as tepoch:\n",
    "        for i, (X_batch, y_batch) in enumerate(tepoch):\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            \n",
    "            # Normalize loss to account for accumulation\n",
    "            loss = loss / accumulation_steps\n",
    "            loss.backward()\n",
    "            \n",
    "            # Only step and zero_grad after accumulation_steps\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                # Add gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            running_loss += loss.item() * accumulation_steps\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "            \n",
    "            tepoch.set_postfix(loss=loss.item() * accumulation_steps, accuracy=correct/total)\n",
    "    \n",
    "    # Step optimizer for remaining gradients\n",
    "    if len(train_loader) % accumulation_steps != 0:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    train_acc = correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {running_loss/len(train_loader):.4f} - Train Accuracy: {train_acc:.4f}\")\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    val_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in val_loader:\n",
    "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "            outputs = model(X_val)\n",
    "            loss = criterion(outputs, y_val)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += y_val.size(0)\n",
    "            val_correct += (predicted == y_val).sum().item()\n",
    "    \n",
    "    val_acc = val_correct / val_total\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    print(f\"Validation Loss: {val_loss:.4f} - Validation Accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "        print(f\"Model saved with validation accuracy: {best_val_acc:.4f}\")\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "# Load the best model for evaluation\n",
    "model.load_state_dict(torch.load('best_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, device, label_encoder):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_test, y_test in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "            outputs = model(X_test)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(y_test.cpu().numpy())\n",
    "    \n",
    "    # Convert numeric labels back to original class names\n",
    "    class_names = label_encoder.classes_\n",
    "    pred_classes = [class_names[i] for i in all_preds]\n",
    "    target_classes = [class_names[i] for i in all_targets]\n",
    "    \n",
    "    # Print classification report\n",
    "    print(classification_report(target_classes, pred_classes))\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(target_classes, pred_classes)\n",
    "    return cm, classification_report(target_classes, pred_classes, output_dict=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 765/765 [00:04<00:00, 181.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      "             Backdoor       1.00      0.93      0.96      4806\n",
      "            DDoS_HTTP       0.94      0.60      0.74      9709\n",
      "            DDoS_ICMP       1.00      0.99      1.00     23286\n",
      "             DDoS_TCP       0.96      0.65      0.77     10012\n",
      "             DDoS_UDP       1.00      1.00      1.00     24314\n",
      "       Fingerprinting       0.23      0.85      0.36       171\n",
      "                 MITM       1.00      1.00      1.00        73\n",
      "               Normal       1.00      1.00      1.00    272801\n",
      "             Password       0.45      0.85      0.59      9987\n",
      "        Port_Scanning       0.50      0.93      0.65      3996\n",
      "           Ransomware       0.95      0.91      0.93      1938\n",
      "        SQL_injection       0.71      0.24      0.36     10166\n",
      "            Uploading       0.68      0.48      0.56      7391\n",
      "Vulnerability_scanner       1.00      0.85      0.92     10006\n",
      "                  XSS       0.34      0.92      0.50      3014\n",
      "\n",
      "             accuracy                           0.94    391670\n",
      "            macro avg       0.78      0.81      0.76    391670\n",
      "         weighted avg       0.96      0.94      0.94    391670\n",
      "\n",
      "Test evaluation complete!\n",
      "Model and evaluation results saved to disk.\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating model on test set...\")\n",
    "cm, report = evaluate(model, test_loader, device, label_encoder)\n",
    "print(\"Test evaluation complete!\")\n",
    "\n",
    "# Save the model and results\n",
    "torch.save(model.state_dict(), 'final_model.pt')\n",
    "joblib.dump(report, 'classification_report.pkl')\n",
    "print(\"Model and evaluation results saved to disk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 1/20: 100%|██████████| 25575/25575 [04:45<00:00, 89.54batch/s, accuracy=0.783, loss=0.39]  \n",
    "Epoch [1/20] - Loss: 0.4509 - Train Accuracy: 0.7830\n",
    "Validation Loss: 0.1123 - Validation Accuracy: 0.9383\n",
    "Model saved with validation accuracy: 0.9383\n",
    "Epoch 2/20: 100%|██████████| 25575/25575 [04:39<00:00, 91.53batch/s, accuracy=0.807, loss=0.422] \n",
    "Epoch [2/20] - Loss: 0.3921 - Train Accuracy: 0.8071\n",
    "Validation Loss: 0.1766 - Validation Accuracy: 0.9363\n",
    "Epoch 3/20: 100%|██████████| 25575/25575 [04:33<00:00, 93.50batch/s, accuracy=0.808, loss=0.394] \n",
    "Epoch [3/20] - Loss: 0.3882 - Train Accuracy: 0.8084\n",
    "Validation Loss: 0.1066 - Validation Accuracy: 0.9406\n",
    "Model saved with validation accuracy: 0.9406\n",
    "Epoch 4/20: 100%|██████████| 25575/25575 [09:31<00:00, 44.73batch/s, accuracy=0.809, loss=0.406]   \n",
    "Epoch [4/20] - Loss: 0.3863 - Train Accuracy: 0.8089\n",
    "Validation Loss: 0.1077 - Validation Accuracy: 0.9397\n",
    "Epoch 5/20: 100%|██████████| 25575/25575 [04:34<00:00, 93.20batch/s, accuracy=0.809, loss=0.436] \n",
    "Epoch [5/20] - Loss: 0.3853 - Train Accuracy: 0.8091\n",
    "Validation Loss: 0.1478 - Validation Accuracy: 0.9337\n",
    "Epoch 6/20: 100%|██████████| 25575/25575 [04:21<00:00, 97.83batch/s, accuracy=0.809, loss=0.421] \n",
    "Epoch [6/20] - Loss: 0.3848 - Train Accuracy: 0.8094\n",
    "Validation Loss: 0.1082 - Validation Accuracy: 0.9395\n",
    "Epoch 7/20: 100%|██████████| 25575/25575 [04:35<00:00, 92.80batch/s, accuracy=0.809, loss=0.398]\n",
    "Epoch [7/20] - Loss: 0.3840 - Train Accuracy: 0.8095\n",
    "Validation Loss: 0.1076 - Validation Accuracy: 0.9399\n",
    "Epoch 8/20: 100%|██████████| 25575/25575 [04:39<00:00, 91.63batch/s, accuracy=0.811, loss=0.389]\n",
    "Epoch [8/20] - Loss: 0.3798 - Train Accuracy: 0.8109\n",
    "Validation Loss: 0.1090 - Validation Accuracy: 0.9393\n",
    "Early stopping triggered!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
