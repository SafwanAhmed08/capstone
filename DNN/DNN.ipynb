{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset/processed_dataset.csv\", low_memory=False)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature scaling complete!\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "df = df.drop(columns=[\"Attack_label\", \"Attack_class\"])\n",
    "X_scaled = scaler.fit_transform(df.drop(columns=[\"Attack_type\"]))\n",
    "y = df[\"Attack_type\"]\n",
    "print(\"Feature scaling complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features: 97, Reduced features: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/safwanahmed/Desktop/capstone/DNN/.venv/lib/python3.9/site-packages/sklearn/decomposition/_pca.py:606: RuntimeWarning: divide by zero encountered in matmul\n",
      "  C = X.T @ X\n",
      "/Users/safwanahmed/Desktop/capstone/DNN/.venv/lib/python3.9/site-packages/sklearn/decomposition/_pca.py:606: RuntimeWarning: overflow encountered in matmul\n",
      "  C = X.T @ X\n",
      "/Users/safwanahmed/Desktop/capstone/DNN/.venv/lib/python3.9/site-packages/sklearn/decomposition/_pca.py:606: RuntimeWarning: invalid value encountered in matmul\n",
      "  C = X.T @ X\n",
      "/Users/safwanahmed/Desktop/capstone/DNN/.venv/lib/python3.9/site-packages/sklearn/decomposition/_base.py:148: RuntimeWarning: divide by zero encountered in matmul\n",
      "  X_transformed = X @ self.components_.T\n",
      "/Users/safwanahmed/Desktop/capstone/DNN/.venv/lib/python3.9/site-packages/sklearn/decomposition/_base.py:148: RuntimeWarning: overflow encountered in matmul\n",
      "  X_transformed = X @ self.components_.T\n",
      "/Users/safwanahmed/Desktop/capstone/DNN/.venv/lib/python3.9/site-packages/sklearn/decomposition/_base.py:148: RuntimeWarning: invalid value encountered in matmul\n",
      "  X_transformed = X @ self.components_.T\n",
      "/Users/safwanahmed/Desktop/capstone/DNN/.venv/lib/python3.9/site-packages/sklearn/decomposition/_base.py:155: RuntimeWarning: divide by zero encountered in matmul\n",
      "  X_transformed -= xp.reshape(self.mean_, (1, -1)) @ self.components_.T\n",
      "/Users/safwanahmed/Desktop/capstone/DNN/.venv/lib/python3.9/site-packages/sklearn/decomposition/_base.py:155: RuntimeWarning: overflow encountered in matmul\n",
      "  X_transformed -= xp.reshape(self.mean_, (1, -1)) @ self.components_.T\n",
      "/Users/safwanahmed/Desktop/capstone/DNN/.venv/lib/python3.9/site-packages/sklearn/decomposition/_base.py:155: RuntimeWarning: invalid value encountered in matmul\n",
      "  X_transformed -= xp.reshape(self.mean_, (1, -1)) @ self.components_.T\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=0.99)  # Increased from 0.95 to 0.99\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "print(f\"Original features: {X_scaled.shape[1]}, Reduced features: {X_pca.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (1299563, 64), Validation set: (324891, 64), Testing set: (406114, 64)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "print(f\"Training set: {X_train.shape}, Validation set: {X_val.shape}, Testing set: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/safwanahmed/Desktop/capstone/DNN/.venv/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled training set: (12862388, 64)\n"
     ]
    }
   ],
   "source": [
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "print(f\"Resampled training set: {X_train_resampled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/safwanahmed/Desktop/capstone/DNN/.venv/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New class distribution after targeted SMOTE:\n",
      "{'Normal': 918742, 'DDoS_ICMP': 74516, 'DDoS_HTTP': 31895, 'Uploading': 23832, 'Password': 50000, 'SQL_injection': 32623, 'DDoS_UDP': 77803, 'Backdoor': 15382, 'DDoS_TCP': 32040, 'Port_Scanning': 50000, 'Vulnerability_scanner': 32018, 'Ransomware': 6203, 'XSS': 50000, 'MITM': 50000}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['label_encoder.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert labels to string for encoding\n",
    "y_train_str = y_train.astype(str)\n",
    "y_val_str = y_val.astype(str)\n",
    "y_test_str = y_test.astype(str)\n",
    "\n",
    "# Combine all label sets to preserve unseen classes for encoding\n",
    "combined_labels = np.concatenate([y_train_str, y_val_str, y_test_str])\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(combined_labels)\n",
    "\n",
    "# Transform labels to integers\n",
    "y_train_encoded = label_encoder.transform(y_train_str)\n",
    "y_val_encoded = label_encoder.transform(y_val_str)\n",
    "y_test_encoded = label_encoder.transform(y_test_str)\n",
    "\n",
    "# Create a mapping of label name to encoded value\n",
    "label_to_id = {name: idx for idx, name in enumerate(label_encoder.classes_)}\n",
    "id_to_label = {idx: name for name, idx in label_to_id.items()}\n",
    "\n",
    "# Select underperforming classes to boost\n",
    "underperforming = ['XSS', 'MITM', 'Password', 'Port_Scanning']\n",
    "\n",
    "# Target number of samples for each\n",
    "smote_strategy = {\n",
    "    label_to_id[label]: 50000 for label in underperforming\n",
    "}\n",
    "\n",
    "# Apply SMOTE only to selected classes\n",
    "smote = SMOTE(sampling_strategy=smote_strategy, random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train_encoded)\n",
    "\n",
    "# Check counts\n",
    "print(\"New class distribution after targeted SMOTE:\")\n",
    "print({id_to_label[k]: v for k, v in Counter(y_train_resampled).items()})\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(df['Attack_type'])\n",
    "\n",
    "# Save updated label encoder\n",
    "import joblib\n",
    "joblib.dump(label_encoder, 'label_encoder.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     label_encoder = joblib.load(\"label_encoder.pkl\")\n",
    "#     y_train_tensor = torch.load(\"y_train_tensor.pt\")\n",
    "#     y_val_tensor = torch.load(\"y_val_tensor.pt\")\n",
    "#     y_test_tensor = torch.load(\"y_test_tensor.pt\")\n",
    "# except:\n",
    "#     # Create and fit label encoder\n",
    "#     label_encoder = LabelEncoder()\n",
    "#     y_train_resampled_str = y_train_resampled.astype(str)\n",
    "#     y_val_str = y_val.astype(str)\n",
    "#     y_test_str = y_test.astype(str)\n",
    "    \n",
    "#     y_train_encoded = label_encoder.fit_transform(y_train_resampled_str)\n",
    "#     y_val_encoded = label_encoder.transform(y_val_str)\n",
    "#     y_test_encoded = label_encoder.transform(y_test_str)\n",
    "    \n",
    "#     # Convert to tensors\n",
    "#     y_train_tensor = torch.tensor(y_train_encoded, dtype=torch.long)\n",
    "#     y_val_tensor = torch.tensor(y_val_encoded, dtype=torch.long)\n",
    "#     y_test_tensor = torch.tensor(y_test_encoded, dtype=torch.long)\n",
    "    \n",
    "#     # Save for future use\n",
    "#     joblib.dump(label_encoder, \"label_encoder.pkl\")\n",
    "#     torch.save(y_train_tensor, \"y_train_tensor.pt\")\n",
    "#     torch.save(y_val_tensor, \"y_val_tensor.pt\")\n",
    "#     torch.save(y_test_tensor, \"y_test_tensor.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_resampled shape: (1445054, 64)\n",
      "y_train_resampled shape: (1445054,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train_resampled shape:\", X_train_resampled.shape)\n",
    "print(\"y_train_resampled shape:\", y_train_resampled.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tensor = torch.tensor(y_train_resampled, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val_encoded, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_encoded, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(y_train_tensor, \"y_train_tensor.pt\")\n",
    "torch.save(y_val_tensor, \"y_val_tensor.pt\")\n",
    "torch.save(y_test_tensor, \"y_test_tensor.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train_resampled, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_resampled shape: torch.Size([1445054, 64])\n",
      "y_train_resampled shape: torch.Size([1445054])\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train_resampled shape:\", X_train_tensor.shape)\n",
    "print(\"y_train_resampled shape:\", y_train_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_classes = np.unique(y_train_resampled)\n",
    "class_weights = compute_class_weight('balanced', classes=unique_classes, y=y_train_resampled)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512  # Increased from 128 to 512\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulation_steps = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedDNN(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, dropout_rate=0.3):\n",
    "        super(ImprovedDNN, self).__init__()\n",
    "        \n",
    "        # Wider architecture with batch normalization\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.bn4 = nn.BatchNorm1d(64)\n",
    "        self.fc5 = nn.Linear(64, num_classes)\n",
    "\n",
    "        # Activation & regularization\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.act = nn.LeakyReLU(0.1)  # LeakyReLU instead of ReLU\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.act(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.act(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.act(self.bn4(self.fc4(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/safwanahmed/Desktop/capstone/DNN/.venv/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_train_resampled.shape[1]\n",
    "num_classes = len(set(y_train_resampled))\n",
    "\n",
    "model = ImprovedDNN(input_dim, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 2823/2823 [00:19<00:00, 145.89batch/s, accuracy=0.936, loss=0.179] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] - Loss: 0.3265 - Train Accuracy: 0.9356\n",
      "Validation Loss: 0.1129 - Validation Accuracy: 0.9892\n",
      "Model saved with validation accuracy: 0.9892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 2823/2823 [00:18<00:00, 155.15batch/s, accuracy=0.984, loss=0.0834]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20] - Loss: 0.1304 - Train Accuracy: 0.9843\n",
      "Validation Loss: 0.0836 - Validation Accuracy: 0.9927\n",
      "Model saved with validation accuracy: 0.9927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 2823/2823 [00:18<00:00, 155.74batch/s, accuracy=0.986, loss=0.112] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20] - Loss: 0.1089 - Train Accuracy: 0.9864\n",
      "Validation Loss: 0.0737 - Validation Accuracy: 0.9924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 2823/2823 [00:18<00:00, 156.04batch/s, accuracy=0.987, loss=0.0505]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20] - Loss: 0.1012 - Train Accuracy: 0.9872\n",
      "Validation Loss: 0.0856 - Validation Accuracy: 0.9914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 2823/2823 [00:18<00:00, 151.18batch/s, accuracy=0.988, loss=0.337] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20] - Loss: 0.0962 - Train Accuracy: 0.9879\n",
      "Validation Loss: 0.1495 - Validation Accuracy: 0.9879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 2823/2823 [00:18<00:00, 154.93batch/s, accuracy=0.988, loss=0.0458]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20] - Loss: 0.0926 - Train Accuracy: 0.9882\n",
      "Validation Loss: 0.0689 - Validation Accuracy: 0.9926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 2823/2823 [00:18<00:00, 154.69batch/s, accuracy=0.989, loss=0.0664]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20] - Loss: 0.0884 - Train Accuracy: 0.9887\n",
      "Validation Loss: 0.0611 - Validation Accuracy: 0.9937\n",
      "Model saved with validation accuracy: 0.9937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 2823/2823 [00:17<00:00, 160.00batch/s, accuracy=0.989, loss=0.011] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20] - Loss: 0.0859 - Train Accuracy: 0.9889\n",
      "Validation Loss: 0.2998 - Validation Accuracy: 0.9835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 2823/2823 [00:17<00:00, 165.41batch/s, accuracy=0.989, loss=0.0748]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20] - Loss: 0.0833 - Train Accuracy: 0.9892\n",
      "Validation Loss: 0.1790 - Validation Accuracy: 0.9878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 2823/2823 [00:17<00:00, 161.72batch/s, accuracy=0.989, loss=0.0627]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20] - Loss: 0.0831 - Train Accuracy: 0.9893\n",
      "Validation Loss: 0.0613 - Validation Accuracy: 0.9931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 2823/2823 [00:17<00:00, 163.13batch/s, accuracy=0.99, loss=0.0134] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20] - Loss: 0.0810 - Train Accuracy: 0.9895\n",
      "Validation Loss: 0.0572 - Validation Accuracy: 0.9938\n",
      "Model saved with validation accuracy: 0.9938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 2823/2823 [00:18<00:00, 151.47batch/s, accuracy=0.99, loss=0.0833]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20] - Loss: 0.0798 - Train Accuracy: 0.9897\n",
      "Validation Loss: 0.0642 - Validation Accuracy: 0.9926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 2823/2823 [00:17<00:00, 160.75batch/s, accuracy=0.99, loss=0.028] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20] - Loss: 0.0776 - Train Accuracy: 0.9899\n",
      "Validation Loss: 0.0590 - Validation Accuracy: 0.9934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 2823/2823 [00:17<00:00, 163.11batch/s, accuracy=0.99, loss=0.171] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20] - Loss: 0.0763 - Train Accuracy: 0.9901\n",
      "Validation Loss: 0.2360 - Validation Accuracy: 0.9845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 2823/2823 [00:17<00:00, 163.49batch/s, accuracy=0.99, loss=0.0334]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20] - Loss: 0.0770 - Train Accuracy: 0.9901\n",
      "Validation Loss: 0.2085 - Validation Accuracy: 0.9855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 2823/2823 [00:18<00:00, 153.15batch/s, accuracy=0.991, loss=0.0952]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20] - Loss: 0.0714 - Train Accuracy: 0.9909\n",
      "Validation Loss: 0.0556 - Validation Accuracy: 0.9934\n",
      "Early stopping triggered!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "best_val_acc = 0\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    optimizer.zero_grad()  # Reset gradients at the beginning of epoch\n",
    "    \n",
    "    with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\") as tepoch:\n",
    "        for i, (X_batch, y_batch) in enumerate(tepoch):\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            \n",
    "            # Normalize loss to account for accumulation\n",
    "            loss = loss / accumulation_steps\n",
    "            loss.backward()\n",
    "            \n",
    "            # Only step and zero_grad after accumulation_steps\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                # Add gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            running_loss += loss.item() * accumulation_steps\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "            \n",
    "            tepoch.set_postfix(loss=loss.item() * accumulation_steps, accuracy=correct/total)\n",
    "    \n",
    "    # Step optimizer for remaining gradients\n",
    "    if len(train_loader) % accumulation_steps != 0:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    train_acc = correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {running_loss/len(train_loader):.4f} - Train Accuracy: {train_acc:.4f}\")\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    val_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in val_loader:\n",
    "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "            outputs = model(X_val)\n",
    "            loss = criterion(outputs, y_val)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += y_val.size(0)\n",
    "            val_correct += (predicted == y_val).sum().item()\n",
    "    \n",
    "    val_acc = val_correct / val_total\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    print(f\"Validation Loss: {val_loss:.4f} - Validation Accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "        print(f\"Model saved with validation accuracy: {best_val_acc:.4f}\")\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "# Load the best model for evaluation\n",
    "model.load_state_dict(torch.load('best_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, device, label_encoder):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_test, y_test in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "            outputs = model(X_test)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(y_test.cpu().numpy())\n",
    "\n",
    "    class_names = label_encoder.classes_\n",
    "\n",
    "    # Safely map predictions to class names\n",
    "    pred_classes = [class_names[int(i)] for i in all_preds]\n",
    "    target_classes = [class_names[int(i)] for i in all_targets]\n",
    "\n",
    "    print(\"\\n📊 Classification Report:\")\n",
    "    print(classification_report(target_classes, pred_classes))\n",
    "\n",
    "    cm = confusion_matrix(target_classes, pred_classes)\n",
    "    return cm, classification_report(target_classes, pred_classes, output_dict=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 794/794 [00:01<00:00, 456.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Classification Report:\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "             Backdoor       1.00      0.98      0.99      4807\n",
      "            DDoS_HTTP       0.96      0.89      0.93      9967\n",
      "            DDoS_ICMP       1.00      1.00      1.00     23286\n",
      "             DDoS_TCP       0.98      0.92      0.95     10012\n",
      "             DDoS_UDP       1.00      1.00      1.00     24313\n",
      "                 MITM       1.00      0.86      0.92         7\n",
      "               Normal       1.00      1.00      1.00    287107\n",
      "             Password       1.00      1.00      1.00     10016\n",
      "        Port_Scanning       0.82      1.00      0.90      3996\n",
      "           Ransomware       1.00      0.97      0.98      1939\n",
      "        SQL_injection       1.00      1.00      1.00     10195\n",
      "            Uploading       1.00      1.00      1.00      7447\n",
      "Vulnerability_scanner       1.00      1.00      1.00     10006\n",
      "                  XSS       0.72      0.88      0.79      3016\n",
      "\n",
      "             accuracy                           0.99    406114\n",
      "            macro avg       0.96      0.96      0.96    406114\n",
      "         weighted avg       0.99      0.99      0.99    406114\n",
      "\n",
      "Test evaluation complete!\n",
      "Model and evaluation results saved to disk.\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating model on test set...\")\n",
    "cm, report = evaluate(model, test_loader, device, label_encoder)\n",
    "print(\"Test evaluation complete!\")\n",
    "\n",
    "# Save the model and results\n",
    "torch.save(model.state_dict(), 'final_model.pt')\n",
    "joblib.dump(report, 'classification_report.pkl')\n",
    "print(\"Model and evaluation results saved to disk.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
